# Source code

### Contents

- [vcf_generation](#vcf_generation)
    - [run_plink.sh](#run_plinksh)
    - [transpose_matrix.cpp](#transpose_matrix)
    - [duplicate_positions_removal.py](#duplicate_positions_removalpy)
- [biomart_retrieval](#biomart_retrieval)
    - [biomart_snpid_gene_retrieval_v2.R](#biomart_snpid_gene_retrieval_v2r)
- [subsampling](#subsampling)
    - [random_sample_snps_pick.ipynb](#random_sample_snps_pickipynb)
- [data_preprocessing](#data_preprocessing)
    - [data_normality.ipynb](#data_normalityipynb)
    - [mapping_snps_to_GO_v2.py](#mapping_snps_to_gopy)
    - [alternative_prevalence_filtering_test.ipynb](#alternative_prevalence_filtering_testipynb)
    - [alternative_prevalence_filtering.py](#alternative_prevalence_filteringpy)
    - [alternative_prevalence_colsuming.py](#alternative_prevalence_colsumingpy)
- [regression_models](#regression_models)
    - [ML_models.ipynb](#ml_modelsipynb)
    - [aoo_scaler.py](#aoo_scalerpy)
    - [model_results.ipynb](#model_resultsipynb)
    - [model_results_plots.ipynb](#model_results_plotsipynb)
    - [/individual_models](#individual_models)
        - [data_loading.py](#data_loadingpy)
        - [evaluating_functions.py](#evaluating_functionspy)
- [protein_network](#protein_network)
    - [core_genes.py](#core_genespy)
    - [ppi_x_creation.py](#ppi_x_creationpy)
    - [ppi_x_functions.py](#ppi_x_functionspy)

## vcf_generation

### run_plink.sh

To explore regression approaches to our research objective we need features encoded as a table, where each sample is an observation contained in a row, and each feature is a column. The raw enroll data has samples in columns, and SNPs in rows. The first part of the script subsets the vcf files taking only those rows which represent an SNP contained in `data/biomart/revised_filtered_snp_gene_lookup_tab.txt` checking by chromosome and position. This lookup table is generated by [biomart_snpid_gene_retrieval_v2.R](#biomart_snpid_gene_retrieval_v2r).

Then these subsetted rows are translated from the allele encoding of type 0/0 to an integer 0, 1 or 2:

| Old allele encoding | New allele encoding |
|:-------------------:|:-------------------:|
| 0/0 | 0 |
| 0/1 | 1 |
| 1/1 | 2 |

Outside the loop that iterates through chromosomes the script generates intermediate files for each action:
1. Concatenates the subsets of all chromosomes -> data/filtered_enroll/chrompos_snps_model3.txt
2. Transposes the numeric part of the matrix to have samples in columns (by running the build generated by [transpose_matrix.cpp](#transpose_matrix)).
3. Translate the SNPs from chromosome and position to the reference SNP ID.
4. Finally writes the sample names and adds sex and CAG repeat length of each sample as the two first features. These rows are obtained from the metadata enroll file `data/enroll_hd/gwa12345.mis4.9064.sample.info.190805.txt`. Final output is `data/features/feature_matrix_model3.txt`.

Headers in each subsetting and concatenating processes are saved and compared to guarantee we are not mixing up samples and SNPs. A sanity check is also run to ensure 0/0 genotype is the majority.

### transpose_matrix.cpp

Transposes a numeric txt file.

### duplicate_positions_removal.py

In the deature matrix generated by [run_plink.sh](#run_plinksh) there are columns labeled equally due to two reasons: first, in the vcf files there are positions appearing in the same chromosome twice, with different genotypes. These are considered two different SNPs, in two different columns, but are given the same reference ID; second, in the Biomart lookuptable, we got duplicate rows. To avoid any incongruency, these SNPs are totally removed from the feature matrix, saving a filtered version of the matrix called `feature_matrix_m3_filt_0.01_nodups.txt`.

## biomart_retrieval

### biomart_snpid_gene_retrieval_v2.R

Retrieves all SNPs in the chromosome coordinate windows corresponding to the genes that we wish to include in the analysis, detailed in `data/genes/revised_core_genes_names.txt`. This list is formed by all the genes in selected GO terms plus a list of HD modifiers described in literature.
The script outputs a table with the chromosome, position, reference SNP id, alleles and gene name. This table is saved in `data/biomart/revised_snp_gene_lookup_tab.txt`. 

This last file was later modified outside this script to filter out those SNPs which were not single nucleotide variants. The final table used in all applications is `data/biomart/revised_filtered_snp_gene_lookup_tab.txt`

## subsampling

### random_sample_snps_pick.ipynb

Generates random indices for subsetting the feature matrix into toy examples for easy model training. The shape of the subsets is defined by N (sample size) and j (SNPs to be included in the subset). The generated subsets should always follow the proportion of the original matrix to be a good test set. 

The subset samples and snps indeces and names are saved in separate files in `data/features/subsetting`. The subsetting itself is not done in python.

The saved snps indeces always contain the indices of the HD modifiers genes specified in `data/genes/HD_modifiers.txt`.

## data_preprocessing

### data_normality.ipynb

Checks the normality of the outcome variable (age of onset):

- Graphically:
    + Histogram of data behind its Gaussian fit.
    + Q-Q plot comparing with Normal distribution.
- Statistically:
    + Shapiro-Wilk, Kolmogorov-Smirnov, and Scipy's normaltest over the raw y vector.
    + Shapiro-Wilk and Kolmogorov-Smirnov over several transformations of the data (log, reciprocal, exponent, box-cox, Yeo-Johnson, standardization, robust scaler).
    + Dividing the total vector in subsets and checking the percentage of subsets where Shapiro-Wilk test can't prove non-normality.

### mapping_snps_to_GO_v2.sh

Groups SNPs included in feature matrix (contained in `data/features/subsetting/refIDs_row.txt`) into their corresponding genes using the lookupt table from Biomart, and these genes into their GO terms (broad GO terms which were selected for the core genes set, generated by [core_genes.py](#core_genespy).) Saves the results in a table with columns SNP, Gene and GO (`data/SNPs/snps_gene_GO_m3.txt`).

> To do:
Filter what SNPs ultimately affect the protein structure.

### alternative_prevalence_filtering_test.ipynb

We want to get how many SNPs have a given minimum alternative variant prevalence. That is, we want to set a minimum percentage of samples that have an alternative allele at a specific SNP. Having an alternative allele, considering how we have build the feature matrix, means that the encoding of the genotype at an SNP is either 1 (heterozygous, one of the two nucleotides is the alternative variant) or 2 (that position in both chromosomes contains the alternative variant), or which is the same, is different from 0.

A set of values of minimum prevalence is defined, and the number and set of SNPs which achieve this minimum are saved. 

The number of SNPs that achieve each minimum is then plotted against the minimum prevalence values range. With such plot we can decide 

### alternative_prevalence_filtering.py

Python script format of code built in [alternative_prevalence_filtering_test.ipynb](#alternative_prevalence_filtering_testipynb) for the entire feature matrix (X). 

Generates 3 files, saved in data/features/subsetting:
- `snps_filtered_per_val.pkl`: dictionary where keys are minimum prevalence values and values are lists of the SNPs surpassing that minimum.
- `gen_different_from_0_sum.pkl`: vector of the sum of samples with genotype different from 0 for each SNP, following the column order of the feature matrix.
- `prevalence_filtering_sizes.pkl`: dictionary similar to snps_filtered_per_val but values being the total number of SNPs attaining each minimum.

> To do:
Implement the grouping by gene and GO term as done in the test notebook for the entire feature matrix (only done for toy example).

### alternative_prevalence_colsuming.py

Filters the input matrix based on a minimum alternative prevalence threshold. Writes the filtered matrix with the same name as input plus a sufix specifying the threshold. 

Input arguments to the file:

- `matrix`: (str) Input (patient,SNP) matrix path.
- `n_samples`: (int) Number of samples in input matrix.
- `n_snps`: (int) Number of SNPs in input matrix.
- `thres`: (float) Minimum alternative prevalence threshold (as percentage between 0 and 1)

Output saved in `data/features` with the name `<input matrix name>_filt_<thres>.txt`.

## regression_models

### ML_models.ipynb

Training and testing of different ML regressions. Contains feature scaling, train/test separation, balance study, and model evaluation functions.

### aoo_scaler.py

Generates the standard scaler for AOO to be used in the models using all AOO values available. Pickled scaler saved in `data/features/aooStanardScaler.pkl`.

### model_results.ipynb

Extracts the SNPs contributing to the models. For the Lasso case, it takes the coefficients different from 0, retrieves the feature's SNP, gene and GO term, and assembles a table with the results (saved at `data/ml_results/lasso_coefs.txt`).

For the boosting methods, feature importance is sorted by gain, and a similar table to the lasso one is assembled and saved similarly.

### model_results_plots.ipynb

Graphical representation of the results summarized by [model_results.ipynb](#model_results_plotsipynb), producing Manhattan plots which represent the importance of each SNP, having each SNP represented on its relative position inside the corresponding chromosome.

### /individual_models

Subdirectory containing individual scripts to run for each model, plus two source scripts containing functions to load data and evaluate models, explained below. Each individual script saves the model evaluations at `data/ml_results`, plus pickles the model and saves it at `data/ml_results/regressors`.

#### data_loading.py

Source script containing functions for data loading used in regression models in this same directory. The functions are:

- `read_sparse_X`: reads the feature matrix by chunks and loads it into executing stage as a csr matrix of float32 values.
- `scale_CAG`: performs a MinMax scaling of the sparse matrix CAG column.
- `interaction_computation`: multiplies all SNP genotypes (0, 1, or 2) by the scaled CAG value of the corresponding subject. The input is sparse (), the function densifies it by chunks (number of rows given by `chunk_size` input parameter) to compute the interaction between SNP and CAG, and makes it sparse again.

#### evaluating_functions.py

Source script containing functions to visualize the training of a model (`model_description`) and how well can it predict the test set (`model_metrics`). 

- `model_description`: returns a plot of the input regressor predictions of the train set over their real outcomes. The percentage of deviance explained (D<sup>2</sup>) by the model is the subtitle of the plot.
- `model_metrics`: Evaluates regressor predicting test samples and returning: R<sup>2</sup>, MSE and MAE; and a plot formed by two subplots of true vs predicted AOO and the residuals vs predicted.
        
Actual and predicted values are plotted transformed to their original range if the transformation scaler is passed as argument in both functions.
    

## protein_network

### core_genes.py
We concatenated a list of proteins related to GO terms that have a potential relation with HD. The genes corresponding to these proteins are the core genes used in the interactions network. This script searches for the gene symbol (using ExPASy and SwissProt), given that the concatenated core proteins list is given with UniProt labels.

The final core genes file (.csv) is saved in /data/genes/core_genes.csv. Its columns are:
1. `Gene(UniProtKB)`
2. `Gene_name`
3. `GOclass`: specific GO class.
4. `broad_GOclass`: broad GO class which we included in our core because of HD relation potential.
5. `Gene`: gene symbol.

### ppi_x_creation.py
Creation of the gene interaction network. Uses functions created in the script `ppi_x_creation.py`. Imports the table of core genes created in [core_genes.py](#core_genespy). 

By iterating over different thresholds (confidence score of the interaction, computed by StringDB comparing different databases) and including interactions of different order, different networks are created and saved in /data/networks. Each combination of threshold (thr) and order (o) outputs 3 objects:

- `adjM_thr{}_o{}`: adjacency matrix. Numpy matrix.
- `labels_adjM_thr{}_o{}`: list of gene names of the matrix rows and columns, because the matrix has no labels. Pickled.
- `gene_network_thr{}_o{}`: nx object. Pickled.

In each iteration the size of the generated network is saved in the list `sizes`, which is in the end plotted. The resulting plot is found in /data/core_genes_order_growth.png.

### ppi_x_functions.py

Contains functions used in [ppi_x_creation.py](#ppi_x_creationpy):

`get_interactions_string(name, threshold)`: Searches for protein  interactions of the input protein ('name') through StringDB, and returns those interactions with a higher confidence level than     the defined threshold.

`ppi_network_creator(elements, threshold)`: Constructs interaction graph between the proteins in 'elements' obtained with the function get_interactions_string. 

`multiple_order_ppi_interaction(elements,threshold,norder)`: Constructs interactions graphs of a specific interaction order ('norder'). Uses nodes of previous iteration as input for the next graph. The graphs are constructed using the function `ppi_network_creator`.

The concept of interaction order considers the inmediate interactions to the core 'elements' as the first order interactions (so norder = 1 gives inmediate interactions, only 1 String request).

See the docstrings of the functions for more information on the their parameters and outputs.